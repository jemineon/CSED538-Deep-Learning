{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eb0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e40cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러가지 설정값들\n",
    "\n",
    "SEED = 9814\n",
    "\n",
    "batch_size = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "n_fft = 4096\n",
    "win_length = 400\n",
    "hop_length = 160\n",
    "n_mels = 80\n",
    "# n_mfcc = 40\n",
    "sr = 16000\n",
    "\n",
    "input = n_mels\n",
    "output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bb0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(\"Using PyTorch version:\", torch.__version__,' Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b5938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # nn.Module은 모든 neural network의 base class라고 한다. \n",
    "    def __init__(self, input_size, output_size, hidden_sizes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1_2 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2_3 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3_4 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.fc4_5 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        self.fc5_6 = nn.Linear(hidden_sizes[3], output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2_3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3_4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4_5(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.fc5_6(x)\n",
    "\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러가지 utility 함수들\n",
    "\n",
    "def make_metadata_file(metadata_path, target_path, type):\n",
    "    if type == \"train\":\n",
    "        typeid = 1\n",
    "    if type == \"dev\":\n",
    "        typeid = 2\n",
    "    if type == \"test\":\n",
    "        typeid = 3\n",
    "    speaker_id_set = set()\n",
    "    cherrypick_list = []\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            strid, path = line.split()\n",
    "            if int(strid) == typeid:\n",
    "                speaker_id_set.add(int(path.split(\"/\")[0][-3:]))\n",
    "                if len(speaker_id_set) == 11: break\n",
    "                cherrypick_list.append(line)\n",
    "\n",
    "    with open(target_path, \"w+\") as f: # 기존 내용은 지워짐\n",
    "        f.write(\"\".join(cherrypick_list))\n",
    "\n",
    "    return len(speaker_id_set)\n",
    "\n",
    "def get_num_speakers(path):\n",
    "    speaker_id_set = set()\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            speaker_id_set.add(int(line.split()[1].split(\"/\")[0][-3:]))\n",
    "\n",
    "    print(speaker_id_set)\n",
    "    return len(speaker_id_set)\n",
    "\n",
    "# torchaudio의 transforms.MFCC를 이용한 mfcc 추출 함수 반환\n",
    "def get_mfcc_transform(sr, n_mfcc, n_fft, n_mels, win_length, hop_length):\n",
    "    return transforms.MFCC(\n",
    "        sample_rate=sr,\n",
    "        n_mfcc=n_mfcc,\n",
    "        melkwargs={\n",
    "            \"n_fft\": n_fft,\n",
    "            \"n_mels\": n_mels,\n",
    "            \"win_length\": win_length,\n",
    "            \"hop_length\": hop_length,\n",
    "            \"mel_scale\": \"htk\"\n",
    "        })\n",
    "\n",
    "# torchaudio의 transforms.MelSpectrogram을 이용한 mel 추출 함수 반환\n",
    "def get_mels_transform(sr, n_fft, n_mels, win_length, hop_length):\n",
    "    return transforms.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "        )\n",
    "\n",
    "def normalize_dataset(x, mean=None, std=None):\n",
    "    if mean is None:\n",
    "        mean = np.mean(x, axis=0)\n",
    "    if std is None:\n",
    "        std = np.std(x, axis=0)\n",
    "        \n",
    "    x = (x - mean) / std\n",
    "\n",
    "    return x, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_metadata_file(\"iden_split.txt\", \"train_moredata_list2.txt\", \"train\")\n",
    "make_metadata_file(\"iden_split.txt\", \"val_list2.txt\", \"dev\")\n",
    "make_metadata_file(\"iden_split.txt\", \"test_list2.txt\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxceleb 데이터셋 다운 및 dataset 객체 얻기\n",
    "train_dataset = torchaudio.datasets.VoxCeleb1Identification(root=\"./\", subset=\"train\", download=True, meta_url=\"train_moredata_list2.txt\")\n",
    "test_dataset = torchaudio.datasets.VoxCeleb1Identification(root=\"./\", subset=\"test\", download=True, meta_url=\"test_list2.txt\")\n",
    "\n",
    "train_dataset_size = int(len(train_dataset) * 0.80)\n",
    "validation_dataset_size = len(train_dataset) - train_dataset_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_dataset_size, validation_dataset_size], generator=generator)\n",
    "\n",
    "print(\"train size: \", train_dataset.__len__())\n",
    "print(\"val size: \", val_dataset.__len__())\n",
    "print(\"test size: \", test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a640bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelDataset(Dataset):\n",
    "    def __init__(self, dataloader, type, mean=0, std=0):\n",
    "        self.dataloader = dataloader\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.sr = dataloader.dataset.__getitem__(0)[1]\n",
    "        print(\"detected sample rate: \", self.sr)\n",
    "        self.get_features = nn.Sequential(\n",
    "            get_mels_transform(self.sr, n_fft, n_mels, win_length, hop_length),\n",
    "            torchaudio.transforms.AmplitudeToDB(),\n",
    "        )\n",
    "        print(f\"====================== ({type}) Generating Mel dataset ======================\")\n",
    "        for idx, sample in enumerate(dataloader):\n",
    "            wave = torch.flatten(sample[0]) # Tensor\n",
    "            id = sample[2] # Tensor\n",
    "            mels = self.get_features(wave)\n",
    "            mels = mels.transpose(0, 1) # (len, n_mfcc)으로 변경\n",
    "\n",
    "            # mels = torch.mean(mels, dim=0)\n",
    "            \n",
    "            converted_mel_list = []\n",
    "            for i in range(mels.shape[0] // 25):\n",
    "                converted_mel_list.append(torch.mean(mels[i*25:i*25+50, :], dim=0))\n",
    "            labels = [id] * len(converted_mel_list) # id를 len만큼 늘림\n",
    "            self.x_train += converted_mel_list\n",
    "            self.y_train += labels\n",
    "        \n",
    "        if type == \"train\":\n",
    "            self.x_train, mean, std = normalize_dataset(self.x_train)\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "        else:\n",
    "            self.x_train, _, _ = normalize_dataset(self.x_train, self.mean, self.std)\n",
    "        print(f\"====================== ({type}) x: \", np.shape(self.x_train), \"y: \", np.shape(self.y_train), \" ======================\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_train[index], self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련에 쓰일 mels feature DataLoader 생성\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4, prefetch_factor=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, prefetch_factor=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, prefetch_factor=2)\n",
    "\n",
    "train_meldataset = MelDataset(train_dataloader, \"train\")\n",
    "val_meldataset = MelDataset(val_dataloader, \"val\", train_meldataset.mean, train_meldataset.std)\n",
    "test_meldataset = MelDataset(test_dataloader, \"test\", train_meldataset.mean, train_meldataset.std)\n",
    "\n",
    "mels_train_dataset = DataLoader(train_meldataset, batch_size=batch_size, shuffle=True)\n",
    "mels_val_dataset = DataLoader(val_meldataset, batch_size=batch_size, shuffle=True)\n",
    "mels_test_dataset = DataLoader(test_meldataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch in mels_val_dataset:\n",
    "    print(batch)\n",
    "    print(f\"mels shape: {batch[0].shape}\")\n",
    "    print(f\"id shape: {batch[1].shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio backend가 있는 지 확인\n",
    "print(str(torchaudio.list_audio_backends()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DCMLP_r2 import DCMLPr2\n",
    "from DCMLP_r3 import DCMLPr3\n",
    "from DCMLP_r3_deep import DeepDCMLPr3\n",
    "from MLP_r2 import MLPr2\n",
    "from MLP_r3 import MLPr3\n",
    "from MLP_r3 import DeepMLPr3\n",
    "from MLP_deep import DeepMLP\n",
    "\n",
    "def lr_lambda(step):\n",
    "    decay_steps = 5 * 10**6 # 5 million\n",
    "    decay_factor = 0.1\n",
    "    return decay_factor ** (step // decay_steps)\n",
    "\n",
    "# model = DCMLPr2(input, output, (512, 256, 256, 256)).to(DEVICE)\n",
    "# model = DCMLPr3(input, output, (256, 256, 256, 256)).to(DEVICE)\n",
    "model = DeepDCMLPr3(input, output, 128).to(DEVICE)\n",
    "# model = MLPr3(input, output, (512, 256, 256, 256)).to(DEVICE)\n",
    "# model = MLPr2(input, output, (512, 256, 256, 256)).to(DEVICE)\n",
    "# model = DeepMLP(input, output, 128).to(DEVICE)\n",
    "# model = DeepMLPr3(input, output, 128).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce2f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_decay(model, train_loader, optimizer, log_interval, scheduler, epoch):\n",
    "    model.train()\n",
    "    train_loss_sum = train_correct = train_total = 0\n",
    "    total_train_batch = len(train_loader)\n",
    "\n",
    "    for batch_idx, (mels, label) in enumerate(train_loader):\n",
    "        mels = mels.to(DEVICE)\n",
    "        label = torch.flatten(label) - 3 # label을 1차원으로 바꾸고 3~12번 사이의 label들을 0~9 범위로 맞춤\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mels)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        train_total += label.size(0)\n",
    "        train_correct += ((torch.argmax(output, 1) == label)).sum().item()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(mels),\n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "            \n",
    "    train_avg_loss = train_loss_sum / total_train_batch\n",
    "    train_avg_acc = 100 * train_correct / train_total\n",
    "\n",
    "    return (train_avg_loss, train_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23edfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss_sum = test_correct = test_total = 0\n",
    "    total_test_batch = len(test_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for mels, label in test_loader:\n",
    "            mels = mels.to(DEVICE)\n",
    "            label = torch.flatten(label) - 3 # label을 1차원으로 바꾸고 3~12번 사이의 label들을 0~9 범위로 맞춤\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(mels)\n",
    "            \n",
    "            test_loss_sum += criterion(output, label).item()\n",
    "\n",
    "            test_total += label.size(0)\n",
    "            test_correct += ((torch.argmax(output, 1) == label)).sum().item()\n",
    "    \n",
    "    test_avg_loss = test_loss_sum / total_test_batch\n",
    "    test_avg_acc = 100 * test_correct / test_total\n",
    "    return (test_avg_loss, test_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e59c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_rate = [0.1, 0.4, 0.6, 0.8]\n",
    "\n",
    "# MLP-r=2\n",
    "\n",
    "# parameters_to_prune = (\n",
    "#     (model.fc1_2, 'weight'),\n",
    "\n",
    "#     (model.fc2_3_i, 'weight'),\n",
    "\n",
    "#     (model.fc2_3_ii, 'weight'),\n",
    "\n",
    "#     (model.fc3_4_i, 'weight'),\n",
    "\n",
    "#     (model.fc3_4_ii, 'weight'),\n",
    "\n",
    "#     (model.fc4_5, 'weight'),\n",
    "\n",
    "#     (model.fc5_6, 'weight'),\n",
    "\n",
    "# )\n",
    "\n",
    "# MLP-r=3\n",
    "# parameters_to_prune = (\n",
    "#     (model.fc1_2, 'weight'),\n",
    "#     (model.fc2_3_i, 'weight'),\n",
    "#     (model.fc2_3_ii, 'weight'),\n",
    "# \t(model.fc2_3_iii, 'weight'),\n",
    "#     (model.fc3_4_i, 'weight'),\n",
    "#     (model.fc3_4_ii, 'weight'),\n",
    "#     (model.fc4_5, 'weight'),\n",
    "#     (model.fc5_6, 'weight'),\n",
    "# )\n",
    "\n",
    "# DeepMLP-r=3\n",
    "# parameters_to_prune = (\n",
    "#     (model.layers[\"fc1_2\"], 'weight'),\n",
    "#     (model.layers[\"fc2_3\"], 'weight'),\n",
    "#     (model.layers[\"fc3_4\"], 'weight'),\n",
    "#     (model.layers[\"fc4_5\"], 'weight'),\n",
    "#     (model.layers[\"fc5_6\"], 'weight'),\n",
    "#     (model.layers[\"fc6_7\"], 'weight'),\n",
    "#     (model.layers[\"fc7_8\"], 'weight'),\n",
    "#     (model.layers[\"fc8_9\"], 'weight'),\n",
    "#     (model.layers[\"fc9_10\"], 'weight'),\n",
    "#     (model.layers[\"fc10_11\"], 'weight'),\n",
    "#     (model.layers[\"fc11_12\"], 'weight'),\n",
    "#     (model.layers[\"fc12_13\"], 'weight'),\n",
    "#     (model.layers[\"fc13_14\"], 'weight'),\n",
    "#     (model.layers[\"fc14_15\"], 'weight'),\n",
    "#     (model.layers[\"fc2_3_ii\"], 'weight'),\n",
    "#     (model.layers[\"fc2_3_iii\"], 'weight'),\n",
    "#     (model.layers[\"fc6_7_ii\"], 'weight')\n",
    "# )\n",
    "\n",
    "# DCMLP-r=2\n",
    "# parameters_to_prune = (\n",
    "#     (model.fc1_2, \"weight\"),\n",
    "#     (model.fc2_3, \"weight\"),\n",
    "#     (model.fc2_4, \"weight\"),\n",
    "#     (model.fc3_4, \"weight\"),\n",
    "#     (model.fc3_5, \"weight\"),\n",
    "#     (model.fc4_5, \"weight\"),\n",
    "#     (model.fc5_6, \"weight\"),\n",
    "# )\n",
    "\n",
    "# DCMLP-r=3\n",
    "# parameters_to_prune = (\n",
    "#     (model.fc1_2, \"weight\"),\n",
    "#     (model.fc2_3, \"weight\"),\n",
    "#     (model.fc2_4, \"weight\"),\n",
    "# \t(model.fc2_5, \"weight\"),\n",
    "#     (model.fc3_4, \"weight\"),\n",
    "#     (model.fc3_5, \"weight\"),\n",
    "#     (model.fc4_5, \"weight\"),\n",
    "#     (model.fc5_6, \"weight\"),\n",
    "# )\n",
    "\n",
    "# DeepDCMLPr3\n",
    "# parameters_to_prune = (\n",
    "#     (model.layers[\"fc1_2\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_3\"], \"weight\"),\n",
    "#     (model.layers[\"fc3_4\"], \"weight\"),\n",
    "#     (model.layers[\"fc4_5\"], \"weight\"),\n",
    "#     (model.layers[\"fc5_6\"], \"weight\"),\n",
    "#     (model.layers[\"fc6_7\"], \"weight\"),\n",
    "#     (model.layers[\"fc7_8\"], \"weight\"),\n",
    "#     (model.layers[\"fc8_9\"], \"weight\"),\n",
    "#     (model.layers[\"fc9_10\"], \"weight\"),\n",
    "#     (model.layers[\"fc10_11\"], \"weight\"),\n",
    "#     (model.layers[\"fc11_12\"], \"weight\"),\n",
    "#     (model.layers[\"fc12_13\"], \"weight\"),\n",
    "#     (model.layers[\"fc13_14\"], \"weight\"),\n",
    "#     (model.layers[\"fc14_15\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_10_i\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_14_i\"], \"weight\"),\n",
    "#     (model.layers[\"fc6_14_i\"], \"weight\")\n",
    "# )\n",
    "\n",
    "# DeepDCMLP-r=2toAny\n",
    "# parameters_to_prune = (\n",
    "#     (model.layers[\"fc1_2\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_3\"], \"weight\"),\n",
    "#     (model.layers[\"fc3_4\"], \"weight\"),\n",
    "#     (model.layers[\"fc4_5\"], \"weight\"),\n",
    "#     (model.layers[\"fc5_6\"], \"weight\"),\n",
    "#     (model.layers[\"fc6_7\"], \"weight\"),\n",
    "#     (model.layers[\"fc7_8\"], \"weight\"),\n",
    "#     (model.layers[\"fc8_9\"], \"weight\"),\n",
    "#     (model.layers[\"fc9_10\"], \"weight\"),\n",
    "#     (model.layers[\"fc10_11\"], \"weight\"),\n",
    "#     (model.layers[\"fc11_12\"], \"weight\"),\n",
    "#     (model.layers[\"fc12_13\"], \"weight\"),\n",
    "#     (model.layers[\"fc13_14\"], \"weight\"),\n",
    "#     (model.layers[\"fc14_15\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_4\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_5\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_6\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_7\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_8\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_9\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_10\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_11\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_12\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_13\"], \"weight\"),\n",
    "#     (model.layers[\"fc2_14\"], \"weight\")\n",
    "# )\n",
    "\n",
    "# DeepDCMLP-r=Anyto14\n",
    "parameters_to_prune = (\n",
    "    (model.layers[\"fc1_2\"], \"weight\"),\n",
    "    (model.layers[\"fc2_3\"], \"weight\"),\n",
    "    (model.layers[\"fc2_14\"], \"weight\"),\n",
    "    (model.layers[\"fc3_4\"], \"weight\"),\n",
    "    (model.layers[\"fc3_14\"], \"weight\"),\n",
    "    (model.layers[\"fc4_5\"], \"weight\"),\n",
    "    (model.layers[\"fc4_14\"], \"weight\"),\n",
    "    (model.layers[\"fc5_6\"], \"weight\"),\n",
    "    (model.layers[\"fc5_14\"], \"weight\"),\n",
    "    (model.layers[\"fc6_7\"], \"weight\"),\n",
    "    (model.layers[\"fc6_14\"], \"weight\"),\n",
    "    (model.layers[\"fc7_8\"], \"weight\"),\n",
    "    (model.layers[\"fc7_14\"], \"weight\"),\n",
    "    (model.layers[\"fc8_9\"], \"weight\"),\n",
    "    (model.layers[\"fc8_14\"], \"weight\"),\n",
    "    (model.layers[\"fc9_10\"], \"weight\"),\n",
    "    (model.layers[\"fc9_14\"], \"weight\"),\n",
    "    (model.layers[\"fc10_11\"], \"weight\"),\n",
    "    (model.layers[\"fc10_14\"], \"weight\"),\n",
    "    (model.layers[\"fc11_12\"], \"weight\"),\n",
    "    (model.layers[\"fc11_14\"], \"weight\"),\n",
    "    (model.layers[\"fc12_13\"], \"weight\"),\n",
    "    (model.layers[\"fc12_14\"], \"weight\"),\n",
    "    (model.layers[\"fc13_14\"], \"weight\"),\n",
    "    (model.layers[\"fc14_15\"], \"weight\")\n",
    ")\n",
    "\n",
    "\n",
    "flag=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69f138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_avg_loss, train_avg_acc = train_with_decay(model, mels_train_dataset, optimizer, log_interval = 200, scheduler=scheduler, epoch=epoch)\n",
    "    train_loss_list.append(train_avg_loss)\n",
    "    train_acc_list.append(train_avg_acc)\n",
    "    \n",
    "    val_loss, val_accuracy = evaluate(model, mels_val_dataset, criterion)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_accuracy)\n",
    "\n",
    "    print(\"\\n[EPOCH: {}], \\tTrain Loss: {:.4f}, \\tTrain Accuracy: {:.2f}, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(\n",
    "        epoch, train_avg_loss, train_avg_acc, val_loss, val_accuracy))\n",
    "    \n",
    "    if epoch%(EPOCHS//5) == 0:\n",
    "        if flag >3:\n",
    "            continue\n",
    "        print('prune_step')\n",
    "        if flag == 0:\n",
    "            amount_prune = prune_rate[flag]\n",
    "        else:\n",
    "            amount_prune = 1-(1/(1-prune_rate[flag-1])*(1-prune_rate[flag]))\n",
    "        print('amount to prune: ', amount_prune)\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=amount_prune,\n",
    "        )\n",
    "        flag += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffba527",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"min train loss: {min(train_loss_list)}\")\n",
    "print(f\"max train acc: {max(train_acc_list)}\")\n",
    "print(f\"min val loss: {min(val_loss_list)}\")\n",
    "print(f\"max val acc: {max(val_acc_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fca0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting train, val results\n",
    "\n",
    "def plot_trend(train_loss_list, train_acc_list, val_loss_list, val_acc_list):\n",
    "\tplt.title('Loss trend')\n",
    "\tplt.xlabel('epochs')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.grid()\n",
    "\n",
    "\tplt.plot(train_loss_list, label='train_loss')\n",
    "\tplt.plot(val_loss_list, label='val_loss')\n",
    "\tplt.legend(loc='best')\n",
    "\n",
    "\tplt.show()\n",
    "\n",
    "\tplt.title('Accuracy trend')\n",
    "\tplt.xlabel('epochs')\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.grid()\n",
    "\n",
    "\tplt.plot(train_acc_list, label='train_acc')\n",
    "\tplt.plot(val_acc_list, label='val_acc')\n",
    "\tplt.legend(loc='best')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa38c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trend(train_loss_list, train_acc_list, val_loss_list, val_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
